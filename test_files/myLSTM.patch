--- /data2/chengyi/myproject/SourceCode/models/Res_plus_p3d_plus_LSTM.py
+++ /data2/chengyi/myproject/SourceCode/models/Res_plus_p3d_plus_LSTM.py
@@ -35,8 +35,8 @@
             if i == 0:
                 h_x, c_x = self.lstm(lstm_in)  # h_x: N * hidden, c_x: N * hidden
                 # Long Time Info ===================================
-                previous_information = h_x.unsqueeze(dim=0)
-                previous_usage = torch.tensor([1.]).cuda()
+                # previous_information = h_x.unsqueeze(dim=0)
+                # previous_usage = torch.tensor([1.]).cuda()
                 # Long Time Info ===================================
             else:
                 if self.c_trans:
@@ -45,29 +45,25 @@
                     previous_state = h_x
                 h_x, c_x = self.lstm(lstm_in, (h_x, c_x))  # h_x: N * hidden, c_x: N * hidden
 
+
                 # Long Time Info ===================================
-                # TODO:对于previous_information需要给予权重，越靠近current，权重越低，以防冗余
-                # previous_information = torch.stack(
-                #     [x * y for x, y in zip(previous_information, Low_data_filter[32 - i:])])
-                added_previous_information = torch.sum(torch.stack([x * y for x, y in zip(previous_information,
-                                                                                          previous_usage)]), dim=0) / (
-                                                 torch.sum(previous_usage)) #/ torch.sum(Low_data_filter[32 - i:])
-                # added_previous_information = torch.sum(
-                #     torch.stack([x * y for x, y in zip(previous_information, previous_usage)]), dim=0) / i
+                # added_previous_information = torch.sum(torch.stack([x*y for x,y in zip(previous_information, previous_usage)]),dim=0)/(torch.sum(previous_usage))
+
                 # shape = N * hidden
-
-                previous_information = torch.cat([previous_information, h_x.unsqueeze(dim=0)], dim=0)
+                # previous_information = torch.cat([previous_information, h_x.unsqueeze(dim=0)], dim=0)
                 # Long Time Info ===================================
 
+
+
                 # TODO: h_x - previous_h
-                use = self.fc_use(h_x)  # N * 2: prob: [use_previous, use_current]
+                use = self.fc_use(h_x) # N * 2: prob: [use_previous, use_current]
                 # use = self.fc_use(h_x-previous_state)
 
-                use = F.gumbel_softmax(use, tau=0.5, hard=False)
+                use = F.gumbel_softmax(use, tau=1, hard=False)
 
                 # TODO: 一直用的是False，改为True试试看
                 # Long Time Info ===================================
-                previous_usage = torch.cat([previous_usage, use[:, 1]])
+                # previous_usage = torch.cat([previous_usage, use[:, 1]])
                 # Long Time Info ===================================
                 # matrix multiple, use gumbel softmax
 
@@ -78,19 +74,16 @@
                     # 如果是c-trans是不是应该对c_x做修改呢？
                     # h_x = torch.bmm(torch.stack([previous_state, c_x], dim=-1), use.unsqueeze(dim=-1)).squeeze(-1)
                     c_x = torch.bmm(torch.stack([previous_state, c_x], dim=-1), use.unsqueeze(dim=-1)).squeeze(-1)
-                    c_x = (c_x + added_previous_information) / 2
                     # c_x = c_x + added_previous_information
                 else:
                     h_x = torch.bmm(torch.stack([previous_state, h_x], dim=-1), use.unsqueeze(dim=-1)).squeeze(-1)
                     # LSTM long time information:
+                    # h_x = (h_x + added_previous_information)/2
+                    # 无法拟合：
                     # =============================================================
-                    h_x = (h_x + added_previous_information) / 2
-                    # =============================================================
-                    # 无法拟合：
-                    # # =============================================================
                     # h_x_cat = torch.cat([h_x, added_previous_information], dim=1)
                     # h_x = self.fc_longtime(h_x_cat)
-                    # # =============================================================
+                    # =============================================================
                 # h_x = [h_x_previous, h_x] * use
             cell.append(self.fc_pred_c(c_x))
             hidden.append(self.fc_pred(h_x))